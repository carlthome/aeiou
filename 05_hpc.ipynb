{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp hpc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hpc\n",
    "\n",
    "> routines for running on clusters\n",
    "\n",
    "This part isn't strictly for audio i/o, but...sue me. The point of this package is to reduce code-copying between Harmonai projects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export \n",
    "import yaml\n",
    "import accelerate\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio import transforms as T\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export \n",
    "def get_accel_config(filename='~/.cache/huggingface/accelerate/default_config.yaml'):\n",
    "    \"get huggingface accelerate config info\" \n",
    "    try:  # first try to use the default file\n",
    "        filename = filename.replace('~', str(Path.home()))\n",
    "        with open(filename, 'r') as file:\n",
    "            ac =  yaml.safe_load(file)\n",
    "    except OSError:\n",
    "        ac = {}\n",
    "        \n",
    "    # then update using any environment variables\n",
    "    if os.getenv('MAIN_PROCESS_IP') is not None: ac['main_process_ip'] = os.getenv('MAIN_PROCESS_IP')\n",
    "    if os.getenv('MACHINE_RANK')    is not None: ac['machine_rank']    = os.getenv('MACHINE_RANK')\n",
    "    if os.getenv('NUM_MACHINES')    is not None: ac['num_machines']    = os.getenv('NUM_MACHINES')\n",
    "    if os.getenv('NUM_PROCESSES')   is not None: ac['num_processes']   = os.getenv('NUM_PROCESSES')\n",
    "\n",
    "    return ac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'compute_environment': 'LOCAL_MACHINE',\n",
       " 'deepspeed_config': {},\n",
       " 'distributed_type': 'MULTI_GPU',\n",
       " 'fsdp_config': {},\n",
       " 'machine_rank': 0,\n",
       " 'main_process_ip': '',\n",
       " 'main_process_port': 12332,\n",
       " 'main_training_function': 'main',\n",
       " 'mixed_precision': 'no',\n",
       " 'num_machines': 2,\n",
       " 'num_processes': 8,\n",
       " 'use_cpu': False}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ac = get_accel_config('examples/accel_config.yaml')\n",
    "ac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is a little utility to replace `print`, where it'll only print on the cluster headnode. Note that you can only send one string to `hprint`, so use f-strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export        \n",
    "class HostPrinter():\n",
    "    \"lil accelerate utility for only printing on host node\"\n",
    "    def __init__(self, accelerator, tag='\\033[96m', untag='\\033[0m'): #added some colors\n",
    "        self.accelerator, self.tag, self.untag = accelerator, tag, untag\n",
    "    def __call__(self, s:str):\n",
    "        if self.accelerator.is_main_process:\n",
    "            print(self.tag + s + self.untag, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[96mUsing device: cuda\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#test hostprinter\n",
    "accelerator = accelerate.Accelerator()\n",
    "device = accelerator.device\n",
    "hprint = HostPrinter(accelerator)  # hprint only prints on head node\n",
    "hprint(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch+Accelerate Model routines\n",
    "For when the model is wrapped in a `accelerate` accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export \n",
    "def save(accelerator, args, model, opt=None, epoch=None, step=None):\n",
    "    \"for checkpointing & model saves\"\n",
    "    accelerator.wait_for_everyone()\n",
    "    filename = f'{args.name}_{step:08}.pth' if (step is not None) else f'{args.name}.pth'\n",
    "    if accelerator.is_main_process:\n",
    "        tqdm.write(f'Saving to {filename}...')\n",
    "    obj = {'model': accelerator.unwrap_model(model).state_dict() }\n",
    "    if opt is not None:   obj['opt'] = opt.state_dict()\n",
    "    if epoch is not None: obj['epoch'] = epoch\n",
    "    if step is not None:  obj['step'] = step\n",
    "    accelerator.save(obj, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils for Accelerate of Lightning\n",
    "Be sure to use \"unwrap\" any accelerate model when calling these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export \n",
    "def n_params(\n",
    "    module # raw PyTorch model/module, e.g. returned by accelerator.unwrap_model()\n",
    "    ):\n",
    "    \"\"\"Returns the number of trainable parameters in a module.\n",
    "    Be sure to use accelerator.unwrap_model when calling this.  \"\"\"\n",
    "    return sum(p.numel() for p in module.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export \n",
    "def freeze(\n",
    "    model  # raw PyTorch model, e.g. returned by accelerator.unwrap_model()\n",
    "    ):\n",
    "    \"\"\"freezes model weights; turns off gradient info\n",
    "    If using accelerate, call thisaccelerator.unwrap_model when calling this.  \"\"\"\n",
    "    for param in model.parameters():  \n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import nbdev_export\n",
    "nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
