{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe09744",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b703b37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b778b6",
   "metadata": {},
   "source": [
    "# datasets\n",
    "> Routines for loading/handling datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9414fc4",
   "metadata": {},
   "source": [
    "Many of these routines are dupes or mods from \"audio-diffusion\" repo by Zach Evans w/ contributions by Scott Hawley https://github.com/zqevans/audio-diffusion/blob/main/diffusion/utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a097942",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb91a8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "from torchaudio import transforms as T\n",
    "from torchvision import transforms as VT\n",
    "import random\n",
    "import os\n",
    "import tqdm\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "from aeiou.core import load_audio, get_audio_filenames, is_silence\n",
    "from fastcore.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79380ec",
   "metadata": {},
   "source": [
    "## Augmentation routines\n",
    "\n",
    "Not all of these are used.  Code copied from https://github.com/zqevans/audio-diffusion/blob/main/diffusion/utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33582de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class PadCrop(nn.Module):\n",
    "    def __init__(self, \n",
    "        n_samples,           # length of chunk to extract from longer signal\n",
    "        randomize=True,      # draw cropped chunk from a random position in audio file\n",
    "        redraw_silence=True, # a chunk containing silence will be replaced with a new one\n",
    "        silence_thresh=-60,  # threshold in dB below which we declare to be silence\n",
    "        max_redraws=2        # when redrawing silences, don't do it more than this many\n",
    "        ):\n",
    "        super().__init__()\n",
    "        store_attr()     # sets self.___ vars automatically\n",
    "    \n",
    "    def draw_chunk(self, signal):\n",
    "        \"here's the part that actually draws a cropped/padded chunk of audio from signal\"\n",
    "        n, s = signal.shape\n",
    "        start = 0 if (not self.randomize) else torch.randint(0, max(0, s - self.n_samples) + 1, []).item()\n",
    "        end = start + self.n_samples\n",
    "        chunk = signal.new_zeros([n, self.n_samples])\n",
    "        chunk[:, :min(s, self.n_samples)] = signal[:, start:end]\n",
    "        return chunk\n",
    "    \n",
    "    def __call__(self, signal):\n",
    "        \"when part of the pipline, this will grab a padded/cropped chunk from signal\"\n",
    "        chunk = self.draw_chunk(signal)\n",
    "        num_redraws = 0\n",
    "        while self.redraw_silence and is_silence(chunk, thresh=self.silence_thresh) and (num_redraws < self.max_redraws):\n",
    "            #print(f\"    PadCrop: Got silence.  Redrawing. Try {num_redraws+1} of {self.max_redraws}\")\n",
    "            chunk, num_redraws = self.draw_chunk(signal), num_redraws+1\n",
    "        return chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c5b3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export    \n",
    "class PhaseFlipper(nn.Module):\n",
    "    \"she was PHAAAAAAA-AAAASE FLIPPER, a random invert yeah\"\n",
    "    def __init__(self, \n",
    "        p=0.5  # probability that phase flip will be applied\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "    def __call__(self, signal):\n",
    "        return -signal if (random.random() < self.p) else signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541f69c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export  \n",
    "class FillTheNoise(nn.Module):\n",
    "    \"randomly adds a bit of noise, just to spice things up\"\n",
    "    def __init__(self, \n",
    "        p=0.33       # probability that noise will be added\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "    def __call__(self, signal):\n",
    "        return signal + 0.25*random.random()*(2*torch.rand_like(signal)-1) if (random.random() < self.p) else signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38ec0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export    \n",
    "class RandPool(nn.Module):\n",
    "    def __init__(self, p=0.2):\n",
    "        self.p, self.maxkern = p, 100\n",
    "    def __call__(self, signal):\n",
    "        if (random.random() < self.p):\n",
    "            ksize = int(random.random()*self.maxkern)\n",
    "            avger = nn.AvgPool1d(kernel_size=ksize, stride=1, padding=1)\n",
    "            return avger(signal)\n",
    "        else:\n",
    "            return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd6b527",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class NormInputs(nn.Module):\n",
    "    \"Normalize inputs to [-1,1]. Useful for quiet inputs\"\n",
    "    def __init__(self, \n",
    "        do_norm=True    # controllable parameter for turning normalization on/off\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.do_norm = do_norm\n",
    "        self.eps = 1e-2\n",
    "    def __call__(self, signal):\n",
    "        return signal if (not self.do_norm) else signal/(torch.amax(signal,-1)[0] + self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00ce813",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export    \n",
    "class Mono(nn.Module):\n",
    "    \"convert audio to mono\"\n",
    "    def __call__(self, signal):\n",
    "        return torch.mean(signal, dim=0) if len(signal.shape) > 1 else signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cf8d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Stereo(nn.Module):\n",
    "    \"convert audio to stereo\"\n",
    "    def __call__(self, signal):\n",
    "        signal_shape = signal.shape\n",
    "        # Check if it's mono\n",
    "        if len(signal_shape) == 1: # s -> 2, s\n",
    "            signal = signal.unsqueeze(0).repeat(2, 1)\n",
    "        elif len(signal_shape) == 2:\n",
    "            if signal_shape[0] == 1: #1, s -> 2, s\n",
    "                signal = signal.repeat(2, 1)\n",
    "            elif signal_shape[0] > 2: #?, s -> 2,s\n",
    "                signal = signal[:2, :]    \n",
    "        return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6520c9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export    \n",
    "class RandomGain(nn.Module):\n",
    "    \"apply a random gain to audio\"\n",
    "    def __init__(self, min_gain, max_gain):\n",
    "        super().__init__()\n",
    "        self.min_gain = min_gain\n",
    "        self.max_gain = max_gain\n",
    "\n",
    "    def __call__(self, signal):\n",
    "        gain = random.uniform(self.min_gain, self.max_gain)\n",
    "        signal = signal * gain\n",
    "        return signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf846139",
   "metadata": {},
   "source": [
    "## AudioDataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba890cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class AudioDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Reads from a tree of directories and serves up cropped bits from any and all audio files\n",
    "    found therein. For efficiency, best if you \"chunk\" these files via chunkadelic\n",
    "    modified from https://github.com/drscotthawley/audio-diffusion/blob/main/dataset/dataset.py\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "        paths,             # list of strings of directory (/tree) names to draw audio files from\n",
    "        sample_rate=48000, # audio sample rate in Hz\n",
    "        sample_size=65536, # how many audio samples in each \"chunk\"\n",
    "        random_crop=True,  # take chunks from random positions within files\n",
    "        load_frac=1.0,     # fraction of total dataset to load\n",
    "        cache_training_data=False,  # True = pre-load whole dataset into memory (not fully supported)\n",
    "        num_gpus=8,        # used only when `cache_training_data=True`, to avoid duplicates,\n",
    "        redraw_silence=True, # a chunk containing silence will be replaced with a new one\n",
    "        silence_thresh=-60,  # threshold in dB below which we declare to be silence\n",
    "        max_redraws=2,        # when redrawing silences, don't do it more than this many\n",
    "        augs='PhaseFlipper()' # list of augmentation transforms **after PadCrop**, as a string\n",
    "        ):\n",
    "        super().__init__()\n",
    "    \n",
    "        # Note moved augs definition to config file / cmd-line arg. \n",
    "        \"\"\"self.augs = torch.nn.Sequential(\n",
    "          PadCrop(sample_size, randomize=random_crop, ),\n",
    "          #RandomGain(0.7, 1.0),\n",
    "          #RandPool(),\n",
    "          #FillTheNoise(),\n",
    "          PhaseFlipper(),\n",
    "          #NormInputs(do_norm=global_args.norm_inputs),\n",
    "        )\"\"\"\n",
    "        # base_augs are always applied\n",
    "        base_augs = 'PadCrop(sample_size, randomize=random_crop, redraw_silence=redraw_silence, silence_thresh=silence_thresh, max_redraws=max_redraws)'\n",
    "        #self.augs = torch.nn.Sequential(  )\n",
    "        self.augs = eval(f'torch.nn.Sequential( {base_augs}, {augs} )')  \n",
    "\n",
    "        self.silence_thresh = silence_thresh\n",
    "        self.redraw_silence = redraw_silence\n",
    "        self.max_redraws = max_redraws\n",
    "        self.sr = sample_rate\n",
    "        self.cache_training_data = cache_training_data\n",
    "        \n",
    "        self.encoding = torch.nn.Sequential(  # TODO: technically this can be treated as part of an augmentation\n",
    "          #Stereo() # if images can be 3-channel RGB, we can do stereo. \n",
    "          Mono()   # but RAVE expects mono, ....for now ;-) \n",
    "        )\n",
    "\n",
    "        self.filenames = get_audio_filenames(paths)\n",
    "        print(f\"{len(self.filenames)} files found.\")\n",
    "        self.n_files = int(len(self.filenames)*load_frac)\n",
    "        self.filenames = self.filenames[0:self.n_files]\n",
    "        if cache_training_data: self.preload_files()\n",
    "\n",
    "        self.convert_tensor = VT.ToTensor()\n",
    "\n",
    "    def load_file_ind(self, file_list,i): # used when caching training data\n",
    "        return load_audio(file_list[i]).cpu()\n",
    "\n",
    "    def get_data_range(self): # for parallel runs, only grab part of the data -- OBVIATED BY CHUNKING.\n",
    "        start, stop = 0, len(self.filenames)\n",
    "        try:\n",
    "            local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "            world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "            interval = stop//world_size\n",
    "            start, stop = local_rank*interval, (local_rank+1)*interval\n",
    "            return start, stop\n",
    "        except KeyError as e: # we're on GPU 0 and the others haven't been initialized yet\n",
    "            start, stop = 0, len(self.filenames)//self.num_gpus\n",
    "            return start, stop\n",
    "\n",
    "    def preload_files(self):\n",
    "        print(f\"Caching {self.n_files} input audio files:\")\n",
    "        wrapper = partial(self.load_file_ind, self.filenames)\n",
    "        start, stop = self.get_data_range()\n",
    "        with Pool(processes=cpu_count()) as p:   # //8 to avoid FS bottleneck and/or too many processes (b/c * num_gpus)\n",
    "            self.audio_files = list(tqdm.tqdm(p.imap(wrapper, range(start,stop)), total=stop-start))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "    \n",
    "    \n",
    "    def get_next_chunk(self, \n",
    "        idx     # the index of the file within the list of files\n",
    "        ):\n",
    "        \"The heart of this whole dataset routine\"\n",
    "        audio_filename = self.filenames[idx]\n",
    "        try:\n",
    "            if self.cache_training_data:\n",
    "                audio = self.audio_files[idx] # .copy()\n",
    "            else:\n",
    "                audio = load_audio(audio_filename, sr=self.sr)\n",
    "\n",
    "            #Run augmentations on this sample (including random crop)\n",
    "            if self.augs is not None:\n",
    "                audio = self.augs(audio)\n",
    "                \n",
    "            audio = audio.clamp(-1, 1)\n",
    "            \n",
    "            if self.encoding is not None:  # Encode the file to assist in prediction\n",
    "                audio = self.encoding(audio)\n",
    "            return audio\n",
    "        \n",
    "        except Exception as e:\n",
    "          print(f'Error loading file {audio_filename}: {e}')\n",
    "          return None\n",
    "        \n",
    "        \n",
    "        \n",
    "    def __getitem__(self, \n",
    "        idx     # the index of the file within the list of files\n",
    "        ):\n",
    "        audio = self.get_next_chunk(idx)\n",
    "                \n",
    "        # even with PadCrop set to reject silences, it could be that the whole file is silence; \n",
    "        num_redraws = 0 \n",
    "        while (audio is None) or (self.redraw_silence and is_silence(audio, thresh=self.silence_thresh) \\\n",
    "            and (num_redraws < self.max_redraws)):\n",
    "            #print(f\"AudioDataset.__getitem__: Got None or silence (torch.max = {torch.max(audio)})  Redrawing. Attempt {num_redraws+1} of {self.max_redraws}\")\n",
    "            next_idx = random.randint(0,len(self.filenames)-1)     # pick some other file at random\n",
    "            audio, num_redraws = self.get_next_chunk(next_idx), num_redraws+1\n",
    "               \n",
    "        return self[random.randrange(len(self))] if (audio is None) else audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9186b65",
   "metadata": {},
   "source": [
    "Quick check to catch minor errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaa4ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 files found.\n",
      "Resampling examples/example.wav from 44100 Hz to 48000 Hz\n",
      "a = tensor([-0.0003, -0.0004, -0.0006,  ...,  0.0000,  0.0000,  0.0000])\n"
     ]
    }
   ],
   "source": [
    "dataset = AudioDataset('examples/')\n",
    "a = dataset.__getitem__(0)\n",
    "print(\"a =\",a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ac8c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import nbdev_export\n",
    "nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c61b582",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
